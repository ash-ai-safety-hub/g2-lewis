{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from einops import einsum, rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_P_matrix(p):\n",
    "    \"\"\"\n",
    "    P_{kij} = P(Agent 0 observes i, Agent 1 observes j | Currently in state k)\n",
    "    \"\"\"\n",
    "    p1_matrix = torch.tensor([\n",
    "        [1-p,p,0,0],\n",
    "        [p,1-p,0,0],\n",
    "        [0,0,1-p,p],\n",
    "        [0,0,p,1-p]\n",
    "    ], dtype=torch.float32).unsqueeze(-1)\n",
    "    p2_matrix = torch.tensor([\n",
    "        [1-p,0,p,0],\n",
    "        [0,1-p,0,p],\n",
    "        [p,0,1-p,0],\n",
    "        [0,p,0,1-p]\n",
    "    ], dtype=torch.float32).unsqueeze(-1)\n",
    "    return torch.matmul(p1_matrix,p2_matrix.mT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Q_matrix(policy_1, policy_2):\n",
    "    \"\"\"\n",
    "    Q_{lij} = Pr(Going to state l | Agent 0 sees i, Agent 1 sees j)\n",
    "    \"\"\"\n",
    "    p1 = policy_1.unsqueeze(-1)\n",
    "    p2 = policy_2.unsqueeze(-1)\n",
    "    return torch.stack(((1-p1)@(1-p2).T, (1-p1)@p2.T, p1@(1-p2).T, p1@p2.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "tensor([[0.4477, 0.4513, 0.0503, 0.0507],\n",
      "        [0.3188, 0.4822, 0.0792, 0.1198],\n",
      "        [0.2111, 0.4879, 0.0909, 0.2101],\n",
      "        [0.1214, 0.4796, 0.0806, 0.3184]])\n"
     ]
    }
   ],
   "source": [
    "p_i = torch.tensor([0.1, 0.2, 0.3, 0.4]).float()\n",
    "p_j = torch.tensor([0.5, 0.6, 0.7, 0.8]).float()\n",
    "p = 0.01\n",
    "\n",
    "P = generate_P_matrix(p)\n",
    "Q = generate_Q_matrix(p_i, p_j)\n",
    "\n",
    "print(P.dtype)\n",
    "print(Q.dtype)\n",
    "PQ = torch.einsum(\"kij, lij -> kl\", [P, Q])\n",
    "#PQ = torch.tensordot(P, Q)\n",
    "print(PQ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyIPD:\n",
    "    def __init__(self, game, p_i, p_j, p, gamma, optim_index):\n",
    "        \"\"\"\n",
    "        Note: self.game should be 4x2\n",
    "        \"\"\"\n",
    "        self.game = game.float()\n",
    "        self.p_i = torch.nn.Parameter(p_i.float(), requires_grad=True)\n",
    "        self.p_j = p_j.float()\n",
    "        self.p = p\n",
    "        self.gamma = gamma\n",
    "        self.P = generate_P_matrix(self.p)\n",
    "        self.Q = generate_Q_matrix(self.p_i, self.p_j)\n",
    "        self.values_0, self.values_1 = self.find_values()\n",
    "\n",
    "\n",
    "    def find_values(self):\n",
    "        \n",
    "        I = torch.eye(4)  # Identity matrix of size 4x4\n",
    "            \n",
    "        # sum over ij of P_kij * Q_lij\n",
    "\n",
    "        PQ = torch.einsum(\"kij, lij -> kl\", self.P, self.Q)\n",
    "#        self.subtracted_matrix = I - self.gamma * torch.matmul(self.P, self.Q)\n",
    "        self.subtracted_matrix = I - self.gamma * PQ\n",
    "        \n",
    "        inverse_matrix = torch.linalg.solve(self.subtracted_matrix, torch.eye(4))\n",
    "\n",
    "        values = torch.matmul(inverse_matrix, self.game)\n",
    "\n",
    "        return values.T\n",
    "\n",
    "\n",
    "    def optimize_pi(self, num_iterations, learning_rate=0.05):\n",
    "        #  logit_p_i = torch.log(self.p_i / (1 - self.p_i)).clone().detach().requires_grad_(True)  # Logit transformation\n",
    "        logit_p_i = torch.logit(self.p_i).clone().detach().requires_grad_(True)  # Logit transformation\n",
    "\n",
    "        optimizer = torch.optim.Adam([logit_p_i], lr=learning_rate)\n",
    "        storage = {}\n",
    "        for i in range(num_iterations):\n",
    "            #print(f\"\\n Run {i}\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            self.p_i = torch.sigmoid(logit_p_i)\n",
    "            self.Q = generate_Q_matrix(self.p_i, self.p_j)\n",
    "            self.values_0, self.values_1 = self.find_values()\n",
    "\n",
    "            loss = -self.values_0.sum()   # We want to maximize self.values_0, so we negate it for minimization\n",
    "            #  loss.backward(retain_graph=True)\n",
    "            loss.backward()\n",
    "\n",
    "            #print(\"p_i values:\", self.p_i)\n",
    "            #print(\"value for agent i:\", self.values_0)\n",
    "            #print(\"total value for agent i:\", self.values_0.sum())\n",
    "            #print(\"loss:\", loss)  # Check the value of the loss\n",
    "            storage[i] = {}\n",
    "            storage[i][\"p_i\"] = self.p_i\n",
    "            storage[i][\"values_0\"] = self.values_0\n",
    "            storage[i][\"total_value_0\"] = self.values_0.sum()\n",
    "            storage[i][\"loss\"] = loss\n",
    "\n",
    "            optimizer.step()\n",
    "        return storage\n",
    "\n",
    "#            with torch.no_grad():  # We don't want these operations to be tracked in the computational graph\n",
    "#                eps = 1e-7\n",
    "#                logit_p_i = torch.log((self.p_i + eps) / (1 - self.p_i + eps)).clone().detach().requires_grad_(True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prisoners_dilemma = torch.tensor([[[3, 0],\n",
    "                                   [4, 1]],\n",
    "                                   \n",
    "                                  [[3, 4],\n",
    "                                   [0, 1]]])\n",
    "\n",
    "game_1 = NoisyIPD(game = prisoners_dilemma,\n",
    "                        p_i = torch.tensor([0.1, 0.2, 0.3, 0.4]).float(),\n",
    "                        p_j = torch.tensor([0.5, 0.6, 0.7, 0.8]).float(),\n",
    "                        p = p,\n",
    "                        gamma = gamma)\n",
    "\n",
    "storage_vals = game_obj.optimize_pi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
